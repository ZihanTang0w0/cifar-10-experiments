{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97aed175",
   "metadata": {},
   "source": [
    "# Cifar-10 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5da174",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The core architecture is a conditioned **U-Net** designed for image generation on CIFAR-10 ($32 \\times 32$ resolution). The model employs a symmetric encoder-decoder structure with skip connections, utilizing Residual Blocks and Self-Attention layers to capture both local textures and global context.\n",
    "\n",
    "### 1. U-Net Architecture & Funnel\n",
    "The network processes the input through a series of downsampling and upsampling stages. The base channel dimension is set to **128**.\n",
    "\n",
    "| Stage | Resolution | Channels (In $\\to$ Out) | Blocks |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Input** | $32 \\times 32$ | $3 \\to 128$ | Initial Convolution |\n",
    "| **Down 1** | $32 \\times 32 \\to 16 \\times 16$ | $128 \\to 128$ | 2 $\\times$ ResBlock + Strided Conv |\n",
    "| **Down 2** | $16 \\times 16 \\to 8 \\times 8$ | $128 \\to 256$ | 2 $\\times$ ResBlock + Attention + Strided Conv |\n",
    "| **Down 3** | $8 \\times 8 \\to 4 \\times 4$ | $256 \\to 256$ | 2 $\\times$ ResBlock + Attention + Strided Conv |\n",
    "| **Bottleneck**| $4 \\times 4$ | $256$ | ResBlock $\\to$ Attention $\\to$ ResBlock |\n",
    "| **Up 1** | $4 \\times 4 \\to 8 \\times 8$ | $512^* \\to 256$ | Upsample + Concat $\\to$ 2 $\\times$ ResBlock + Attn |\n",
    "| **Up 2** | $8 \\times 8 \\to 16 \\times 16$ | $512^* \\to 256$ | Upsample + Concat $\\to$ 2 $\\times$ ResBlock + Attn |\n",
    "| **Up 3** | $16 \\times 16 \\to 32 \\times 32$ | $384^* \\to 128$ | Upsample + Concat $\\to$ 2 $\\times$ ResBlock |\n",
    "| **Output** | $32 \\times 32$ | $128 \\to 3$ | GroupNorm $\\to$ SiLU $\\to$ Conv |\n",
    "\n",
    "*\\*Note: Input channels in Up stages are higher due to concatenation with skip connections from the Down path.*\n",
    "\n",
    "### 2. Global Conditioning\n",
    "Conditioning is handled via an **additive embedding** approach that combines temporal information and class labels into a unified global context vector. This vector is computed once per forward pass and shared across all network blocks.\n",
    "\n",
    "* **Time Embedding:** Time steps $t$ are projected using fixed sinusoidal embeddings followed by a Multi-Layer Perceptron (MLP).\n",
    "* **Class Embedding:** Class labels $c$ (including a null class for Classifier-Free Guidance) are mapped via a learnable lookup table.\n",
    "* **Combination:** The final conditioning vector $v_{cond}$ is the element-wise sum of the time and class embeddings:\n",
    "    $$v_{cond} = \\text{MLP}(\\text{Sinusoidal}(t)) + \\text{Embedding}(c)$$\n",
    "\n",
    "### 3. Temporal & Class Injection\n",
    "The conditioning vector $v_{cond}$ is injected into the network within every **Residual Block**.\n",
    "\n",
    "1.  **Projection:** Inside the block, the global $v_{cond}$ vector is passed through a SiLU activation and a Linear layer to match the channel dimensions of the current feature map.\n",
    "2.  **Broadcasting & Addition:** The projected embedding is spatially broadcast (repeated across height and width) and added to the feature map after the first convolution but before the second convolution.\n",
    "\n",
    "This mechanism ensures that both the \"when\" (diffusion timestep) and \"what\" (class label) signals modulate the feature processing at every resolution level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf6bce",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Details\n",
    "\n",
    "The model is trained using a unified framework that supports both standard Diffusion (DDPM/DDIM) and Rectified Flow objectives. The training process minimizes the Mean Squared Error (MSE) between the model prediction and the target defined by the physics of the chosen mode.\n",
    "\n",
    "### 1. Forward Process & Objectives\n",
    "The forward process adds noise to the clean image $x_0$ to produce a noisy latent $x_t$ at a random timestep $t \\in [0, T]$.\n",
    "\n",
    "* **Diffusion Mode (DDPM)**\n",
    "    The forward process follows a variance-preserving schedule on a hypersphere:\n",
    "    $$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "    * **Target:** The model is trained to predict the noise $\\epsilon$.\n",
    "\n",
    "* **Flow Mode (Rectified Flow)**\n",
    "    The forward process follows a straight-line interpolation between data $x_0$ and noise $x_1$:\n",
    "    $$x_t = (1 - t) x_0 + t x_1, \\quad x_1 \\sim \\mathcal{N}(0, I)$$\n",
    "    * **Target:** The model is trained to predict the **velocity** $v$ (the direction pointing from data to noise):\n",
    "        $$v = x_1 - x_0$$\n",
    "\n",
    "### 2. Classifier-Free Guidance (CFG)\n",
    "CFG is implemented to enable conditional generation without requiring a separate classifier.\n",
    "\n",
    "* **Training:**\n",
    "    During training, class labels $c$ are randomly dropped and replaced with a null token ($c_\\emptyset$, index 10) with a probability of $p_{drop}$.\n",
    "    $$c_{train} = \\begin{cases} c_\\emptyset & \\text{if } r < 0.1 \\\\ c & \\text{otherwise} \\end{cases}$$\n",
    "    * **Default Drop Probability:** $0.1$ (10%).\n",
    "\n",
    "* **Inference:**\n",
    "    Sampling is performed using a linear combination of conditional and unconditional predictions to amplify the guidance signal.\n",
    "    $$\\hat{prediction} = \\text{pred}(x_t, c_\\emptyset) + s \\cdot (\\text{pred}(x_t, c) - \\text{pred}(x_t, c_\\emptyset))$$\n",
    "    * **Default Inference Scale ($s$):** $3.0$.\n",
    "\n",
    "### 3. Noise Sampling Frequency\n",
    "* **Once per Epoch:** For every image in the dataset, a random timestep $t$ is sampled independently in every epoch.\n",
    "* This ensures the model sees every image at various noise levels throughout the complete training duration.\n",
    "\n",
    "### 4. Optimization\n",
    "* **Optimizer:** `AdamW` is used for weight updates.\n",
    "* **Learning Rate:** `3e-4` ($0.0003$).\n",
    "* **Batch Size:** `128`.\n",
    "* **Betas:** Defaults are used (typically `(0.9, 0.999)`) as no specific overrides are present in the code.\n",
    "\n",
    "### 5. Training Duration\n",
    "* **Epochs:** The model is trained for **100** epochs by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3d4dc",
   "metadata": {},
   "source": [
    "---\n",
    "## Sampling & Inference Details\n",
    "\n",
    "Sampling is performed periodically during training to monitor progress. The model employs a unified sampler that selects the appropriate Ordinary Differential Equation (ODE) solver based on the training mode.\n",
    "\n",
    "### 1. Diffusion Mode: DDIM Sampler\n",
    "For the Diffusion model, the **Denoising Diffusion Implicit Models (DDIM)** sampler is used. This allows for deterministic sampling with fewer steps than the training process.\n",
    "\n",
    "The update step at time $t$ to reach $t-1$ (where $t-1$ represents the next step in the schedule `t_next`) is defined as:\n",
    "\n",
    "$$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\underbrace{\\left( \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\hat{\\epsilon}_\\theta(x_t)}{\\sqrt{\\bar{\\alpha}_t}} \\right)}_{\\text{predicted } x_0} + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\cdot \\hat{\\epsilon}_\\theta(x_t)$$\n",
    "\n",
    "* **Prediction:** The model predicts the noise $\\hat{\\epsilon}_\\theta$ (using CFG).\n",
    "* **Reconstruction:** The \"predicted $x_0$\" is derived by removing the predicted noise from the current step.\n",
    "* **Step:** The process points to $x_{t-1}$ deterministically (setting noise variance $\\sigma=0$).\n",
    "\n",
    "### 2. Flow Mode: Euler Solver\n",
    "For the Rectified Flow model, a first-order **Euler Method** is used to solve the Neural ODE. This treats the generation process as moving along a straight line trajectory from noise to data.\n",
    "\n",
    "The update step is calculated using the time delta $dt = t_{next} - t_{now}$ (which is negative during sampling):\n",
    "\n",
    "$$x_{t_{next}} \\approx x_{t_{now}} + v_\\theta(x_{t_{now}}) \\cdot dt$$\n",
    "\n",
    "* **Prediction:** The model predicts the velocity field $v_\\theta$ (using CFG).\n",
    "* **Step:** The current state is updated by following the velocity vector for the duration of the time step $dt$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5153dac",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation Details\n",
    "\n",
    "Evaluation is performed using the **Frechet Inception Distance (FID)**, which measures the similarity between the distributions of real and generated images in the feature space of a pre-trained InceptionV3 network.\n",
    "\n",
    "### 1. Metric: Frechet Inception Distance (FID)\n",
    "The evaluation script uses a PyTorch-native implementation of FID optimized for GPU.\n",
    "\n",
    "* **Feature Extraction:** Images are resized to $299 \\times 299$ and passed through an **InceptionV3** network (pre-trained on ImageNet). Features are extracted from the penultimate pooling layer (2048 dimensions).\n",
    "* **Formula:** The distance between the Gaussian distributions of the real features $(\\mu_r, \\Sigma_r)$ and generated features $(\\mu_g, \\Sigma_g)$ is calculated as:\n",
    "    $$d^2((\\mu_r, \\Sigma_r), (\\mu_g, \\Sigma_g)) = ||\\mu_r - \\mu_g||^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})$$\n",
    "\n",
    "### 2. Evaluation Scenarios\n",
    "The evaluation script computes FID across five distinct configuration matrices to assess quality, mode collapse, and class separability:\n",
    "\n",
    "* **Standard Global FID (All vs. All):**\n",
    "    * Compares the distribution of the **entire real dataset** against the **entire generated dataset**. This is the standard metric for overall image quality and diversity.\n",
    "\n",
    "* **Class-Specific Diagonal (Real $C_i$ vs. Gen $C_i$):**\n",
    "    * Compares **Real Class $i$** against **Generated Class $i$**. This measures how well the model generates a specific class (e.g., \"Does the generated Dog look like a real Dog?\").\n",
    "\n",
    "* **Cross-Class Matrix (Real $C_i$ vs. Gen $C_j$):**\n",
    "    * Compares **Real Class $i$** against **Generated Class $j$** (where $i \\neq j$). Low values here might indicate class confusion or mode collapse (e.g., generated Cats looking like real Dogs).\n",
    "\n",
    "* **Class-to-Global (Real $C_i$ vs. Gen All):**\n",
    "    * Compares **Real Class $i$** against the **entire generated distribution**. This checks if a specific real mode is represented anywhere in the generated output.\n",
    "\n",
    "* **Internal Separability (Gen $C_i$ vs. Gen $C_j$):**\n",
    "    * Compares **Generated Class $i$** against **Generated Class $j$**. This acts as a diversity check; high values are desired, indicating that the model's generated classes are distinct from one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee9924",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0964cef",
   "metadata": {},
   "source": [
    "### Visualize a few generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46766d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at ./generated_data/generated_seed_101007_1000x10_mode_flow_steps_80_cfg_scale_2.4/generated_batch\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration\n",
    "# ==========================================\n",
    "# Replace this with your specific file path\n",
    "FILE_PATH = './generated_data/generated_seed_101007_1000x10_mode_flow_steps_80_cfg_scale_2.4/generated_batch'\n",
    "\n",
    "# CIFAR-10 Class Names (for labeling the plot)\n",
    "CLASS_NAMES = [\n",
    "    \"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\",\n",
    "    \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 2. Loading Logic\n",
    "# ==========================================\n",
    "def unpickle(file):\n",
    "    \"\"\"Standard CIFAR-10 loading routine.\"\"\"\n",
    "    with open(file, 'rb') as fo:\n",
    "        # encoding='bytes' is required for Python 3\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict\n",
    "\n",
    "def load_and_reshape(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    data_dict = unpickle(file_path)\n",
    "    \n",
    "    raw_data = data_dict[b'data']   # Shape: (N, 3072)\n",
    "    labels = data_dict[b'labels']   # List of N integers\n",
    "    \n",
    "    # Reshape Logic:\n",
    "    # 1. Reshape to (Batch, Channels, Height, Width) -> (N, 3, 32, 32)\n",
    "    # 2. Transpose to (Batch, Height, Width, Channels) -> (N, 32, 32, 3) for Matplotlib\n",
    "    num_images = raw_data.shape[0]\n",
    "    images = raw_data.reshape(num_images, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# ==========================================\n",
    "# 3. Visualization\n",
    "# ==========================================\n",
    "def visualize_grid(images, labels, rows=4, cols=8):\n",
    "    \"\"\"Plots a grid of images with labels.\"\"\"\n",
    "    total_images = rows * cols\n",
    "    \n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.5, rows * 1.5))\n",
    "    fig.suptitle(f\"Samples from: {os.path.basename(os.path.dirname(FILE_PATH))}\", fontsize=10)\n",
    "    \n",
    "    # Flatten axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(total_images):\n",
    "        if i >= len(images):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(images[i])\n",
    "        \n",
    "        # Label\n",
    "        class_idx = labels[i]\n",
    "        class_name = CLASS_NAMES[class_idx]\n",
    "        ax.set_title(f\"{class_name}\", fontsize=8)\n",
    "        \n",
    "        # Remove ticks\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# Main Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    images, labels = load_and_reshape(FILE_PATH)\n",
    "    \n",
    "    if images is not None:\n",
    "        print(f\"Loaded {len(images)} images.\")\n",
    "        \n",
    "        # Visualize a random selection or the first N?\n",
    "        # Let's visualize a mix of classes by sorting or just taking the first few.\n",
    "        # Since your generation script generates 1000 class 0, then 1000 class 1...\n",
    "        # we should sample with a stride to see different classes.\n",
    "        \n",
    "        stride = max(1, len(images) // 32) \n",
    "        indices = [i * stride for i in range(32)]\n",
    "        \n",
    "        # Gather samples\n",
    "        sample_imgs = images[indices]\n",
    "        sample_lbls = [labels[i] for i in indices]\n",
    "        \n",
    "        visualize_grid(sample_imgs, sample_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56505bd",
   "metadata": {},
   "source": [
    "## Draw the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8805ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs generated successfully.\n",
      "Graphs generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graphs(csv_path, output_dir=None):\n",
    "    if output_dir is None:\n",
    "        output_dir = \".\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    # 1. Load Data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Set visual theme\n",
    "    sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # Graph 1: Global FID Analysis\n",
    "    # ==========================================\n",
    "    # 1a. FID vs CFG Scale (Hue = Steps)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.lineplot(\n",
    "        data=df, \n",
    "        x=\"cfg_scale\", \n",
    "        y=\"global_fid\", \n",
    "        hue=\"steps\", \n",
    "        style=\"mode\",\n",
    "        palette=\"viridis\",\n",
    "        markers=True, dashes=False, linewidth=2.5, markersize=9\n",
    "    )\n",
    "    plt.title(\"Global FID vs. CFG Scale\")\n",
    "    plt.ylabel(\"Global FID (Lower is Better)\")\n",
    "    plt.xlabel(\"CFG Scale\")\n",
    "    plt.savefig(f\"{output_dir}/graph1a_fid_vs_cfg.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 1b. FID vs Sampling Steps (Hue = CFG)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.lineplot(\n",
    "        data=df, \n",
    "        x=\"steps\", \n",
    "        y=\"global_fid\", \n",
    "        hue=\"cfg_scale\", \n",
    "        style=\"mode\",\n",
    "        palette=\"rocket_r\", # Light = Low CFG, Dark = High CFG\n",
    "        markers=True, dashes=False, linewidth=2.5, markersize=9\n",
    "    )\n",
    "    plt.title(\"Global FID vs. Sampling Steps\")\n",
    "    plt.ylabel(\"Global FID (Lower is Better)\")\n",
    "    plt.xlabel(\"Sampling Steps\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', title=\"CFG Scale\")\n",
    "    plt.savefig(f\"{output_dir}/graph1b_fid_vs_steps.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # ==========================================\n",
    "    # Graph 2: Quality vs. Diversity Trade-off\n",
    "    # ==========================================\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.scatterplot(\n",
    "        data=df, \n",
    "        x=\"avg_diversity_ratio\", \n",
    "        y=\"avg_class_fid\", \n",
    "        hue=\"cfg_scale\", \n",
    "        style=\"mode\",\n",
    "        size=\"steps\", \n",
    "        sizes=(100, 300),\n",
    "        palette=\"flare\",\n",
    "        edgecolor=\"black\", alpha=0.8\n",
    "    )\n",
    "    # Reference Line for Perfect Diversity\n",
    "    plt.axvline(1.0, color='green', linestyle='--', linewidth=2, label=\"Ideal Diversity (1.0)\")\n",
    "    \n",
    "    plt.title(\"Quality (FID) vs. Diversity (Trace Ratio)\")\n",
    "    plt.xlabel(\"Diversity Ratio (Gen Variance / Real Variance)\")\n",
    "    plt.ylabel(\"Quality: Avg Class-wise FID (Lower is Better)\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.savefig(f\"{output_dir}/graph2_quality_diversity.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # ==========================================\n",
    "    # Graph 3: Separability\n",
    "    # ==========================================\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.lineplot(\n",
    "        data=df, \n",
    "        x=\"cfg_scale\", \n",
    "        y=\"avg_separability\", \n",
    "        hue=\"steps\", \n",
    "        style=\"mode\",\n",
    "        palette=\"magma\",\n",
    "        markers=True, dashes=False, linewidth=2.5, markersize=9\n",
    "    )\n",
    "    \n",
    "    # Add GT Baseline if available\n",
    "    if \"gt_separability\" in df.columns:\n",
    "        gt_val = df[\"gt_separability\"].iloc[0]\n",
    "        plt.axhline(gt_val, color='red', linestyle='--', linewidth=2, label=f\"GT Baseline ({gt_val:.2f})\")\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.title(\"Class Separability (Cross-Class FID)\")\n",
    "    plt.ylabel(\"Avg Separability (Higher is Better)\")\n",
    "    plt.xlabel(\"CFG Scale\")\n",
    "    plt.savefig(f\"{output_dir}/graph3_separability.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Graphs generated successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    draw_graphs('./eval_results/eval_batch_flow_1000/results.csv', output_dir=\"./graphs/eval_batch_flow_1000\")\n",
    "    draw_graphs('./eval_results/eval_batch_diffusion_1000/results.csv', output_dir=\"./graphs/eval_batch_diffusion_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d8fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
